\documentclass[12pt,a4paper]{article}

\usepackage[UTF8]{ctex}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{xcolor}

\usetikzlibrary{shapes.geometric, arrows.meta, positioning, fit, backgrounds, calc, decorations.pathreplacing}

\geometry{left=1.5cm, right=1.5cm, top=2cm, bottom=2cm}

\definecolor{hdfscolor}{RGB}{255,193,7}
\definecolor{mapcolor}{RGB}{76,175,80}
\definecolor{combinecolor}{RGB}{33,150,243}
\definecolor{shufflecolor}{RGB}{156,39,176}
\definecolor{reducecolor}{RGB}{244,67,54}
\definecolor{outputcolor}{RGB}{255,152,0}

\tikzset{
    process/.style={
        rectangle,
        rounded corners,
        minimum width=3.5cm,
        minimum height=1cm,
        text centered,
        draw=black,
        line width=0.8pt,
        font=\footnotesize\bfseries
    },
    storage/.style={
        cylinder,
        shape border rotate=90,
        aspect=0.25,
        minimum width=3cm,
        minimum height=1.2cm,
        draw=black,
        line width=0.8pt,
        font=\footnotesize\bfseries
    },
    data/.style={
        rectangle,
        minimum width=2.8cm,
        minimum height=0.6cm,
        text centered,
        draw=black,
        line width=0.6pt,
        font=\scriptsize,
        fill=white
    },
    arrow/.style={
        -Stealth,
        line width=1.2pt
    },
    dasharrow/.style={
        -Stealth,
        line width=1pt,
        dashed
    },
    thickarrow/.style={
        -Stealth,
        line width=2pt
    },
    forkbar/.style={
        rectangle,
        fill=black,
        minimum width=0.18cm,
        minimum height=5.2cm
    },
    joinbar/.style={
        rectangle,
        fill=black,
        minimum width=0.18cm,
        minimum height=5.2cm
    }
}

\begin{document}

\title{\textbf{CSI300 因子计算 MapReduce 流水线}}
\author{}
\date{}
\maketitle

\section*{整体架构概览}

本项目使用 Hadoop MapReduce 框架，对 CSI300 指数的 300 只股票的高频快照数据（3秒频）计算 20 个量化因子，并在每个时刻对 300 只股票做截面平均。

\vspace{1em}

\begin{tikzpicture}[node distance=1.2cm and 2cm, scale=0.95, transform shape]

% ========== HDFS Input Layer ==========
\node[storage, fill=hdfscolor!30] (hdfs_input) {HDFS 输入};
\node[data, below=0.3cm of hdfs_input] (input_detail) {
    \begin{tabular}{c}
    300个股票文件 \\
    \texttt{/day/stock/snapshot.csv}
    \end{tabular}
};

% ========== InputFormat ==========
\node[process, fill=hdfscolor!20, below=1.5cm of input_detail] (inputformat) {NonSplittableTextInputFormat};
\node[data, right=0.3cm of inputformat, text width=3cm, align=left] (if_note) {
    禁止文件切分\\
    保证状态连续
};

% ========== Map Phase ==========
\node[process, fill=mapcolor!30, below=1.5cm of inputformat] (mapper) {StockFactorMapper};
\node[data, below=0.3cm of mapper, text width=4cm, align=center] (map_detail) {
    维护 \texttt{lastSnapshot}\\
    计算20个因子\\
    输出: (day,time) $\to$ FactorWritable
};

% Map tasks parallel indication
\node[data, left=0.3cm of mapper, text width=2cm, align=center] (map_parallel) {
    300个\\
    并行任务
};

% ========== Combiner Phase ==========
\node[process, fill=combinecolor!30, below=1.8cm of map_detail] (combiner) {FactorCombiner};
\node[data, below=0.3cm of combiner, text width=4cm, align=center] (combine_detail) {
    本地预聚合\\
    相同 (day,time) 累加\\
    减少 shuffle 数据量
};

% ========== Shuffle & Sort ==========
\node[process, fill=shufflecolor!30, below=1.8cm of combine_detail, minimum width=5cm] (shuffle) {Shuffle \& Sort};
\node[data, below=0.3cm of shuffle, text width=5.5cm, align=center] (shuffle_detail) {
    \textbf{Partitioner}: 按 \texttt{tradingDay} 分区（避免跨日）\\
    \textbf{Sort}: 按 (day, time) 升序排序
};

% ========== Reduce Phase ==========
\node[process, fill=reducecolor!30, below=1.8cm of shuffle_detail] (reducer) {AverageReducer};
\node[data, below=0.3cm of reducer, text width=4.5cm, align=center] (reduce_detail) {
    截面平均: $\bar{f}_i = \frac{\sum f_i}{count}$\\
    MultipleOutputs 按天输出\\
    每天首行写表头
};

\node[data, right=0.3cm of reducer, text width=2cm, align=center] (reduce_count) {
    1个\\
    reducer
};

% ========== OutputFormat ==========
\node[process, fill=outputcolor!20, below=1.5cm of reduce_detail] (outputformat) {ValueOnlyTextOutputFormat};
\node[data, left=0.3cm of outputformat, text width=3cm, align=right] (of_note) {
    纯CSV输出\\
    无key前缀
};

% ========== HDFS Output ==========
\node[storage, fill=outputcolor!30, below=1.5cm of outputformat] (hdfs_output) {HDFS 输出};
\node[data, below=0.3cm of hdfs_output, text width=4cm, align=center] (output_detail) {
    按天分文件:\\
    \texttt{0102.csv, 0103.csv, ...}\\
    格式: tradeTime, alpha\_1..alpha\_20
};

% ========== Arrows ==========
\draw[arrow] (hdfs_input) -- (inputformat);
\draw[arrow] (inputformat) -- (mapper);
\draw[arrow] (mapper) -- (combiner);
\draw[arrow] (combiner) -- (shuffle);
\draw[arrow] (shuffle) -- (reducer);
\draw[arrow] (reducer) -- (outputformat);
\draw[arrow] (outputformat) -- (hdfs_output);

% ========== Side annotations ==========
\node[data, right=4cm of map_detail, text width=3.5cm, align=left, fill=mapcolor!10] (map_logic) {
    \textbf{Map 逻辑:}\\
    1. 跳过表头/空行\\
    2. 解析 Snapshot\\
    3. 换日重置 t-1\\
    4. 计算因子\\
    5. 时间窗口过滤
};

\node[data, right=4cm of reduce_detail, text width=3.5cm, align=left, fill=reducecolor!10] (reduce_logic) {
    \textbf{Reduce 逻辑:}\\
    1. 聚合 FactorWritable\\
    2. 计算平均值\\
    3. 格式化为CSV行\\
    4. 按天写入文件
};

\draw[dasharrow, mapcolor] (mapper.east) -| (map_logic.north);
\draw[dasharrow, reducecolor] (reducer.east) -| (reduce_logic.north);

% ========== Data structure boxes ==========
\node[data, left=4cm of shuffle_detail, text width=3.2cm, align=left, fill=shufflecolor!10] (datastructure) {
    \textbf{数据结构:}\\
    \texttt{DayTimeKey}:\\
    (tradingDay, tradeTime)\\
    \vspace{0.2cm}
    \texttt{FactorWritable}:\\
    factors[20] + count
};

\draw[dasharrow, shufflecolor] (shuffle.west) -| (datastructure.south);

\end{tikzpicture}

\newpage

\section*{详细数据流示意}

\subsection*{1. MapReduce 并行执行示意（Fork/Join）}

\begin{tikzpicture}[node distance=0.9cm and 1.6cm, scale=0.88, transform shape]

% ===== Input side =====
\node[storage, fill=hdfscolor!30] (hdfs_in2) {HDFS 输入};
\node[data, below=0.3cm of hdfs_in2, text width=4.5cm, align=center] (hdfs_in2_detail) {
    \texttt{/day/*/snapshot.csv}\\
    \scriptsize 300 个股票文件（每股每日报价快照）
};
\node[process, fill=hdfscolor!20, below=1.0cm of hdfs_in2_detail, minimum width=5.2cm] (if2) {NonSplittableTextInputFormat};
\node[data, below=0.25cm of if2, text width=5.2cm, align=center] (if2_detail) {
    \scriptsize 每个文件 $\Rightarrow$ 1 个 InputSplit\\
    \scriptsize（禁止切分，保证 t-1 状态连续）
};

% ===== Fork to mapper tasks =====
\node[forkbar, right=2.2cm of if2] (fork_mappers) {};
\node[data, above=0.2cm of fork_mappers, text width=3.0cm, align=center] (fork_label) {
    \textbf{Fork}\\
    \scriptsize YARN 调度 Map Tasks
};

\node[process, fill=mapcolor!25, right=1.8cm of fork_mappers, minimum width=4.2cm, minimum height=0.95cm] (m1) {Mapper\#1：StockFactorMapper};
\node[data, below=0.2cm of m1, text width=4.2cm, align=center] (m1d) {\scriptsize 输入：\texttt{/0102/000001/snapshot.csv}\\\scriptsize 顺序读行 + \texttt{lastSnapshot}};
\node[process, fill=combinecolor!22, right=0.9cm of m1, minimum width=2.8cm, minimum height=0.95cm] (c1) {Combiner};

\node[process, fill=mapcolor!25, below=1.45cm of m1, minimum width=4.2cm, minimum height=0.95cm] (m2) {Mapper\#2：StockFactorMapper};
\node[data, below=0.2cm of m2, text width=4.2cm, align=center] (m2d) {\scriptsize 输入：\texttt{/0102/000002/snapshot.csv}\\\scriptsize 顺序读行 + \texttt{lastSnapshot}};
\node[process, fill=combinecolor!22, right=0.9cm of m2, minimum width=2.8cm, minimum height=0.95cm] (c2) {Combiner};

\node[data, below=1.4cm of m2, text width=4.2cm, align=center] (ellipsis) {\Large$\vdots$\\\scriptsize（中间省略 $\approx 297$ 个 Mapper）};

\node[process, fill=mapcolor!25, below=1.55cm of ellipsis, minimum width=4.2cm, minimum height=0.95cm] (m300) {Mapper\#300：StockFactorMapper};
\node[data, below=0.2cm of m300, text width=4.2cm, align=center] (m300d) {\scriptsize 输入：\texttt{/0102/000300/snapshot.csv}\\\scriptsize 顺序读行 + \texttt{lastSnapshot}};
\node[process, fill=combinecolor!22, right=0.9cm of m300, minimum width=2.8cm, minimum height=0.95cm] (c300) {Combiner};

% Taps on fork bar
\coordinate (forkTap1) at ($(fork_mappers.north)+(0,-0.8cm)$);
\coordinate (forkTap2) at ($(fork_mappers.north)+(0,-2.25cm)$);
\coordinate (forkTap3) at ($(fork_mappers.south)+(0,0.8cm)$);

% ===== Join at shuffle =====
\node[joinbar, right=3.0cm of c2] (join_shuffle) {};
\node[process, fill=shufflecolor!25, right=1.8cm of join_shuffle, minimum width=5.2cm] (shuffle2) {Shuffle \& Sort};
\node[data, below=0.25cm of shuffle2, text width=5.2cm, align=center] (shuffle2d) {
    \scriptsize \textbf{Partition}: DayPartitioner（仅按 day）\\
    \scriptsize \textbf{Sort}: (day,time) 升序\\
    \scriptsize \textbf{Group}: 同 key 的 values 聚合到同一个 reduce 调用
};

% ===== Reducer (configurable parallelism) =====
\node[forkbar, right=2.3cm of shuffle2] (fork_reducers) {};
\node[data, above=0.2cm of fork_reducers, text width=3.2cm, align=center] (fork_reducer_label) {
    \textbf{Fork}\\
    \scriptsize Reducers（\texttt{-Dfactor.reducers=N}）
};

\node[process, fill=reducecolor!25, right=1.8cm of fork_reducers, minimum width=4.2cm] (r0) {Reducer：AverageReducer};
\node[data, below=0.25cm of r0, text width=4.2cm, align=center] (r0d) {\scriptsize 输入：key=(day,time)\\\scriptsize values $\approx$ 300 个股票的因子};
\node[process, fill=outputcolor!22, right=1.0cm of r0, minimum width=4.1cm] (outfmt2) {ValueOnlyTextOutputFormat};
\node[storage, fill=outputcolor!30, right=1.0cm of outfmt2] (hdfs_out2) {HDFS 输出};
\node[data, below=0.25cm of hdfs_out2, text width=4.8cm, align=center] (hdfs_out2d) {
    \scriptsize MultipleOutputs 按天输出\\
    \texttt{0102.csv-r-00000}\\
    \texttt{0103.csv-r-00000} \ldots
};

% ===== Connections =====
\draw[arrow] (hdfs_in2) -- (if2);
\draw[dasharrow] (hdfs_in2_detail) -- (if2);
\draw[arrow] (if2) -- (fork_mappers);

\draw[arrow] (forkTap1) -- (m1.west);
\draw[arrow] (forkTap2) -- (m2.west);
\draw[arrow] (forkTap3) -- (m300.west);

\draw[arrow] (m1) -- (c1);
\draw[arrow] (m2) -- (c2);
\draw[arrow] (m300) -- (c300);

% Multiple mappers join to shuffle (network fan-in)
\coordinate (joinTap1) at ($(join_shuffle.north)+(0,-0.8cm)$);
\coordinate (joinTap2) at ($(join_shuffle.center)$);
\coordinate (joinTap3) at ($(join_shuffle.south)+(0,0.8cm)$);
\draw[dasharrow, shufflecolor] (c1.east) -- (joinTap1);
\draw[dasharrow, shufflecolor] (c2.east) -- (joinTap2);
\draw[dasharrow, shufflecolor] (c300.east) -- (joinTap3);
\draw[arrow] (join_shuffle) -- (shuffle2);

\draw[arrow] (shuffle2) -- (fork_reducers);
\coordinate (forkRTap) at ($(fork_reducers.center)$);
\draw[arrow] (forkRTap) -- (r0.west);
\draw[arrow] (r0) -- (outfmt2);
\draw[arrow] (outfmt2) -- (hdfs_out2);
\draw[dasharrow] (hdfs_out2) -- (hdfs_out2d);

% Braces / annotations
\draw[decorate, decoration={brace, amplitude=6pt}, line width=0.8pt]
    ($(m1.north west)+(-0.3cm,0.35cm)$) -- ($(m300.south west)+(-0.3cm,-0.35cm)$)
    node[midway, xshift=-1.2cm, rotate=90, font=\scriptsize] {Map 并行：$\approx 300$ 个 Task（每文件一个）};

\draw[decorate, decoration={brace, amplitude=6pt}, line width=0.8pt]
    ($(r0.north west)+(-0.3cm,0.35cm)$) -- ($(r0.south west)+(-0.3cm,-0.35cm)$)
    node[midway, xshift=-1.0cm, rotate=90, font=\scriptsize] {默认 $N=1$};

\end{tikzpicture}

\newpage

\subsection*{2. 单个 Mapper 的处理流程（顺序读行 + 状态 t-1）}

\begin{tikzpicture}[node distance=0.8cm, scale=0.9, transform shape]

% Title
\node[text width=15cm, align=center, font=\large\bfseries] (title) {
    单个 Mapper 的处理流程（以股票 000063 为例）
};

% Input file
\node[below=0.5cm of title, data, fill=hdfscolor!20, minimum width=8cm, minimum height=1cm] (file) {
    \texttt{/0102/000063/snapshot.csv}\\
    \scriptsize (约4903行，每行包含10档价量数据)
};

% Row-by-row processing
\node[below=0.8cm of file, process, fill=mapcolor!20, minimum width=6cm] (parse) {逐行解析};

\node[below=0.5cm of parse, data, text width=10cm, align=left] (row_example) {
    \texttt{20240102,092500,...,254100,200,254200,12700,...}\\
    $\downarrow$\\
    \texttt{Snapshot}\{tradingDay=20240102, tradeTime=92500, bp1=254100, bv1=200, ap1=254200, av1=12700, ...\}
};

% Factor calculation
\node[below=0.8cm of row_example, process, fill=mapcolor!30, minimum width=6cm] (compute) {计算20个因子};

\node[below=0.3cm of compute, data, text width=11cm, align=left] (factors) {
    \texttt{Snapshot.compute(current, previous)}:\\
    \quad alpha\_1 = ap1 - bp1 = 100.0\\
    \quad alpha\_3 = (ap1 + bp1)/2 = 254150.0\\
    \quad alpha\_17 = ap1 - ap1\_prev（需要 \texttt{lastSnapshot}）\\
    \quad ...（共20个）\\
    $\downarrow$\\
    \texttt{double[] factors = \{100.0, 0.000393, 254150.0, ...\}}
};

% Emit
\node[below=1cm of factors, process, fill=mapcolor!30, minimum width=6cm] (emit) {输出键值对};

\node[below=0.3cm of emit, data, text width=10cm, align=left] (kv) {
    Key: \texttt{DayTimeKey(20240102, 92500)}\\
    Value: \texttt{FactorWritable(factors[20], count=1)}
};

% Time filter
\node[right=1.5cm of emit, data, text width=3.5cm, align=center, fill=yellow!20] (filter) {
    \textbf{时间窗口过滤}\\
    只输出:\\
    09:30 - 11:30\\
    13:00 - 15:00
};

% Combiner aggregation
\node[below=1.2cm of kv, process, fill=combinecolor!30, minimum width=6cm] (combine_local) {Combiner 本地聚合};

\node[below=0.3cm of combine_local, data, text width=10cm, align=left] (combine_ex) {
    相同 (day,time) 的多条记录累加:\\
    \texttt{sum.add(val1); sum.add(val2); ...}\\
    $\downarrow$\\
    \texttt{FactorWritable(sumFactors[20], totalCount)}
};

% Arrows
\draw[arrow] (file) -- (parse);
\draw[arrow] (parse) -- (row_example.north);
\draw[arrow] (row_example.south) -- (compute);
\draw[arrow] (compute) -- (factors.north);
\draw[arrow] (factors.south) -- (emit);
\draw[arrow] (emit) -- (kv.north);
\draw[arrow] (kv.south) -- (combine_local);
\draw[arrow] (combine_local) -- (combine_ex.north);

\draw[dasharrow] (emit.east) -- (filter.west);

\end{tikzpicture}

\vspace{2cm}

\subsection*{3. Shuffle/Sort：按 (day,time) 分组（Join 前的对齐）}

\begin{tikzpicture}[node distance=0.9cm and 1.4cm, scale=0.88, transform shape]

% Title line
\node[text width=15cm, align=center, font=\normalsize\bfseries] (stitle) {
    示例：3 个 Mapper（实际 $\approx$ 300）在同一时刻对齐到同一个 reduce(key)
};

% Mapper outputs (toy example)
\node[process, fill=mapcolor!25, below=0.6cm of stitle, minimum width=4.0cm] (sm1) {Mapper A 输出};
\node[data, below=0.25cm of sm1, text width=4.0cm, align=left] (sm1d) {
    \scriptsize (0102,093000) $\to$ (sum[20],count=1)\\
    \scriptsize (0102,093003) $\to$ (sum[20],count=1)\\
    \scriptsize \ldots
};

\node[process, fill=mapcolor!25, right=0.9cm of sm1, minimum width=4.0cm] (sm2) {Mapper B 输出};
\node[data, below=0.25cm of sm2, text width=4.0cm, align=left] (sm2d) {
    \scriptsize (0102,093000) $\to$ (sum[20],count=1)\\
    \scriptsize (0102,093003) $\to$ (sum[20],count=1)\\
    \scriptsize \ldots
};

\node[process, fill=mapcolor!25, right=0.9cm of sm2, minimum width=4.0cm] (sm3) {Mapper C 输出};
\node[data, below=0.25cm of sm3, text width=4.0cm, align=left] (sm3d) {
    \scriptsize (0102,093000) $\to$ (sum[20],count=1)\\
    \scriptsize (0102,093003) $\to$ (sum[20],count=1)\\
    \scriptsize \ldots
};

% Network shuffle box
\node[process, fill=shufflecolor!25, below=2.2cm of sm2, minimum width=6.2cm] (net) {Shuffle（网络传输）};
\node[data, below=0.25cm of net, text width=6.2cm, align=center] (netd) {
    \scriptsize 多路 fan-in：相同 key 的记录会被路由到同一个 reducer 分区
};

\node[process, fill=shufflecolor!25, below=1.2cm of netd, minimum width=6.2cm] (sortg) {Sort \& Group（分区内排序 + 按 key 分组）};
\node[data, below=0.25cm of sortg, text width=6.2cm, align=left] (sortgd) {
    \scriptsize 分区：DayPartitioner(day)\\
    \scriptsize 排序：DayTimeKey(day,time) 升序\\
    \scriptsize 分组：同 (day,time) 形成 \texttt{Iterable<FactorWritable>}
};

% Reduce call example
\node[process, fill=reducecolor!25, right=1.6cm of sortg, minimum width=4.6cm] (rcall) {reduce(key, values)};
\node[data, below=0.25cm of rcall, text width=4.6cm, align=left] (rcalld) {
    \scriptsize key=(0102,093000)\\
    \scriptsize values=[FW×300]\\
    \scriptsize $\Rightarrow$ 求和 + 平均
};

% Connections
\draw[dasharrow, shufflecolor] (sm1d.south) -- (net.north west);
\draw[dasharrow, shufflecolor] (sm2d.south) -- (net.north);
\draw[dasharrow, shufflecolor] (sm3d.south) -- (net.north east);
\draw[arrow] (net) -- (sortg);
\draw[arrow] (sortg) -- (rcall);
\draw[dasharrow] (sortgd) -- (rcalld);

\draw[decorate, decoration={brace, amplitude=6pt}, line width=0.8pt]
    ($(sm1.north west)+(-0.3cm,0.35cm)$) -- ($(sm3.south east)+(0.3cm,-0.35cm)$)
    node[midway, yshift=0.8cm, font=\scriptsize] {实际为 $\approx 300$ 个 Mapper 并行输出};

\end{tikzpicture}

\vspace{1.2cm}

\subsection*{4. Reducer 聚合与输出（Join：300 股截面平均）}

\begin{tikzpicture}[node distance=0.8cm, scale=0.9, transform shape]

% Title
\node[text width=15cm, align=center, font=\large\bfseries] (title2) {
    Reducer 聚合与输出流程
};

% Input from shuffle
\node[below=0.5cm of title2, data, fill=shufflecolor!20, minimum width=10cm, minimum height=1cm] (shuffle_in) {
    来自 300 个 Mapper 的数据（已按 (day, time) 排序）\\
    \scriptsize Key: (20240102, 93000) → Values: [FactorWritable×300]
};

% Aggregation
\node[below=0.8cm of shuffle_in, process, fill=reducecolor!30, minimum width=6cm] (aggregate) {截面聚合};

\node[below=0.3cm of aggregate, data, text width=11cm, align=left] (agg_detail) {
    \texttt{for (FactorWritable val : values) \{ sum.add(val); \}}\\
    $\downarrow$\\
    \texttt{sum.factors[i]} 累积了300只股票在该时刻的因子总和\\
    \texttt{sum.count} = 300
};

% Average calculation
\node[below=0.8cm of agg_detail, process, fill=reducecolor!30, minimum width=6cm] (average) {计算平均};

\node[below=0.3cm of average, data, text width=10cm, align=left] (avg_detail) {
    对每个因子: $\bar{\alpha}_i = \frac{\sum_{stock=1}^{300} \alpha_i(stock)}{300}$\\
    $\downarrow$\\
    \texttt{averages[] = \{100.023, 0.000392, 254148.5, ...\}}
};

% CSV formatting
\node[below=0.8cm of avg_detail, process, fill=reducecolor!30, minimum width=6cm] (csv_format) {格式化CSV行};

\node[below=0.3cm of csv_format, data, text width=10cm, align=left] (csv_detail) {
    \texttt{"093000,100.023,0.000392,254148.5,..."}
};

% MultipleOutputs
\node[below=0.8cm of csv_detail, process, fill=outputcolor!30, minimum width=7cm] (multi_out) {MultipleOutputs 按天写入};

\node[below=0.3cm of multi_out, data, text width=11cm, align=left] (out_files) {
    当 \texttt{tradingDay} 变化时，写表头到新文件\\
    输出到: \texttt{0102.csv-r-00000}（含表头）\\
    \texttt{tradeTime,alpha\_1,alpha\_2,...,alpha\_20}\\
    \texttt{093000,100.023,0.000392,...}\\
    \texttt{093003,100.018,0.000395,...}\\
    ...
};

% Arrows
\draw[arrow] (shuffle_in) -- (aggregate);
\draw[arrow] (aggregate) -- (agg_detail.north);
\draw[arrow] (agg_detail.south) -- (average);
\draw[arrow] (average) -- (avg_detail.north);
\draw[arrow] (avg_detail.south) -- (csv_format);
\draw[arrow] (csv_format) -- (csv_detail.north);
\draw[arrow] (csv_detail.south) -- (multi_out);
\draw[arrow] (multi_out) -- (out_files.north);

\end{tikzpicture}

\newpage

\section*{关键设计要点}

\subsection*{1. 状态维护（Mapper 中的 lastSnapshot）}

因子 alpha\_17, alpha\_18, alpha\_19 依赖上一时刻的数据：

\begin{align*}
\alpha_{17} &= ap1_t - ap1_{t-1} \\
\alpha_{18} &= \frac{1}{2}[(ap1_t + bp1_t) - (ap1_{t-1} + bp1_{t-1})] \\
\alpha_{19} &= \frac{\sum bv_t}{\sum av_t} - \frac{\sum bv_{t-1}}{\sum av_{t-1}}
\end{align*}

因此 Mapper 必须：
\begin{itemize}
    \item 维护 \texttt{lastSnapshot} 变量
    \item 按时间顺序处理同一股票的所有记录
    \item 遇到换日时重置 \texttt{lastSnapshot = null}
\end{itemize}

\subsection*{2. 文件不切分（NonSplittableTextInputFormat）}

如果 Hadoop 将一个股票文件切成多个 split：
\begin{itemize}
    \item Split1 的 Mapper 处理前半部分 $\checkmark$
    \item Split2 的 Mapper 处理后半部分 $\times$（无法获取 Split1 的最后一条作为 t-1）
\end{itemize}

解决方案：强制每个文件作为一个整体由单个 Mapper 处理。

\subsection*{3. 按天分区（DayPartitioner）}

\texttt{DayPartitioner} 只根据 \texttt{tradingDay} 计算分区号，确保：
\begin{itemize}
    \item 同一天的所有数据进入同一个 Reducer
    \item 不同天的数据不会混算
    \item Reducer 可以安全地按天输出独立文件
\end{itemize}

\subsection*{4. 时间窗口过滤（Mapper 中的 shouldEmit）}

虽然原始数据包含全天的快照，但标准答案只要求输出交易时段：
\begin{itemize}
    \item 上午：09:30:00 - 11:30:00
    \item 下午：13:00:00 - 15:00:00
\end{itemize}

\textbf{注意}：即使某条记录不在输出窗口，也要维护 \texttt{lastSnapshot}，因为 09:30:00 的 t-1 可能来自 09:29:57。

\subsection*{5. 数据流量统计（实际运行结果）}

\begin{center}
\begin{tabular}{|l|r|}
\hline
\textbf{阶段} & \textbf{记录数/数据量} \\
\hline
Map input records & 1,470,900 行 \\
Map output records & 1,440,600 行（过滤后） \\
Map output bytes & 259 MB \\
\hline
Combine input/output & 1,440,600 行 \\
Shuffle bytes & 263 MB \\
\hline
Reduce input groups & 4,802 个 (day,time) 组合 \\
Reduce input records & 1,440,600 行 \\
\hline
HDFS bytes written & 1.76 MB（5天CSV） \\
\hline
\end{tabular}
\end{center}

\subsection*{6. 并行度}

\begin{itemize}
    \item \textbf{Mapper 数量}: 300（等于输入文件数，无法改变）
    \item \textbf{Combiner 实例}: 300（每个 Mapper 后自动运行）
    \item \textbf{Reducer 数量}: 1（可通过 \texttt{-Dfactor.reducers=N} 调整）
\end{itemize}

单 Reducer 的优点：
\begin{itemize}
    \item 每个交易日只写一次表头
    \item 避免多个 Reducer 输出需要后续合并
    \item 对于 4802 个时间点，单 Reducer 足够
\end{itemize}

\end{document}
